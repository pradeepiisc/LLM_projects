{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad2791a-251f-4fe6-a139-c44359709e80",
   "metadata": {},
   "source": [
    "# Summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67fcb9-016c-4eaf-a29d-37079e1af344",
   "metadata": {},
   "source": [
    "In this notebook, we do the following: \n",
    "1. Look at some sample summaries by models like GPT2, T5, BART and PEGASUS\n",
    "2. Build an encoder-decoder model to condense dialogues between several people into a crisp summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a951b1-b7a5-423e-9bac-fa4d3f6f90a0",
   "metadata": {},
   "source": [
    "## Load the CNN/DailyMail dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc9381c-d1b1-4c5b-bcea-1d17f3b29035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/WMYFHCK/.cache/huggingface/datasets/cnn_dailymail/default/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a003986ce17d491cb09320540f9b5b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['article', 'highlights', 'id']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\n",
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c034d1c-0c7b-4784-a8e0-bc2cb4c996fc",
   "metadata": {},
   "source": [
    "#### Sample article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933d11a5-222e-42db-aa6b-eb936e62c2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article (excerpt of 500 characters, total length: 4051):\n",
      "\n",
      "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most s\n",
      "\n",
      "Summary (length: 281):\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n",
    "\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a5505-72eb-473b-8941-f4b16410614d",
   "metadata": {},
   "source": [
    "## Text Summarisation pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e5238-aea5-4f7d-8006-70d996b99442",
   "metadata": {},
   "source": [
    "#### Generate output for same input length for comparable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c77251-2665-42c8-ac2b-1bfdf4c708d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "# We'll collect the generated summaries of each model in a dictionary\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590c850-8dd2-41ee-bcfd-a56da23deeff",
   "metadata": {},
   "source": [
    "#### New line sentences for different sentences in the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602d51f-968c-443f-a297-e7f5f12eee56",
   "metadata": {},
   "source": [
    "##### Using sentence tokenizer from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab93b6bf-fd11-4ad8-a868-8bfc19f33345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WMYFHCK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The U.S. are a country.', 'The U.N. is an organization.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "string = \"The U.S. are a country. The U.N. is an organization.\"\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f658ebf6-66fd-4e57-a2e9-c650a990bd6a",
   "metadata": {},
   "source": [
    "#### Baseline summary - first 3 sentences of the original text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b03f97a-eb4d-4406-9de4-5a29c2aa0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d385a9b4-6604-4c66-8ded-8022efad3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac5de1d-88f8-4d4b-b358-a3d8dc7b2c3e",
   "metadata": {},
   "source": [
    "### Summary using GPT2, not using GPT2-xl as it creates problem while loading\n",
    "GPT-2 Never been trained on CNN data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a39a8a-8e37-479f-8b31-7dc66ed1b043",
   "metadata": {},
   "source": [
    "##### Adding TL;DR at the end gives an indication to gpt2 model to generate summary for the text preceding TL;DR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d5116-eebc-4284-9a00-090c181f81be",
   "metadata": {},
   "source": [
    "##### the pipeline output also has the original text so the actual summary starts post that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be41f0db-97e6-4c29-b661-c7779c45a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c746953f-31f9-4de4-b8b0-d3b3de7d57ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\modeling_utils.py:429: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  with safe_open(checkpoint_file, framework=\"pt\") as f:\n",
      "C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\generation\\utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "summaries[\"gpt2\"] = \"\\n\".join(\n",
    "    sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c09e13-f8f0-4523-8d30-c73ee4604128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA jail has been designated as \"forgotten.\"\\nAs we explained in our earlier post titled \"forgotten,\" inmates have to be removed from their cells at least three times a week if their condition deteriorates.\\nBut, what exactly qualifies as a \"forgotten floor\" means less than an hour to a day.\\nBecause of this, detainees tend to stay far more quiet and have'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[\"gpt2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de423cf3-8e6b-4264-bca7-01ffa738b8b0",
   "metadata": {},
   "source": [
    "##### note the pipeline's output initial paragraph is same as the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d57bdd3d-f75f-4af6-96ac-0c103e2b5965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_out[0][\"generated_text\"][:len(gpt2_query)] == sample_text + \"\\nTL;DR:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730be705-b48a-462e-aef0-5b930ae70c53",
   "metadata": {},
   "source": [
    "### Summary from T5\n",
    "##### Create a universal transformer by formulating all tasks as text to text tasks.\n",
    "##### T5 checkpoints are trained on a mixture of unsupervised data and supervised tasks like summarization\n",
    "##### T5 has been fine tuned on this task(summarisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d55ea-b63a-4cfe-b731-822f457a03cc",
   "metadata": {},
   "source": [
    "##### Input format for summarisation and other tasks like translation\n",
    "\"summarize: \\<ARTICLE>\"\n",
    "\n",
    "NOTE that if we load T5 with pipeline function, we don't need to prepend with summarize:  \n",
    "\n",
    "\"translate English to German: \\<TEXT>\"\n",
    "\n",
    "T5 is a versatile model able to perform multiple tasks with one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e84c039e-5d82-47a1-8a35-cedd99bd2fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d090890251b4182ae665d95020ac5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da0e55333d545a8bc843cb703438219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5f6930eaef41f88934364b0b243705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05345b02999347c0b22916543a9f540c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ab94808cc54cfb90bc986c3df828b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7465f5e00a714210b2189ab49859b5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-small\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68c2cffa-5786-4c50-a3e5-825223631f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"inmates with most severe mental illnesses are incarcerated until they're ready to appear in court .\\nmost often, they face drug charges or charges of assaulting an officer .\\nthey end up on the ninth floor severely mentally disturbed, but not getting real help .\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[\"t5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101ba94-a9a9-457b-84b2-261d7f648240",
   "metadata": {},
   "source": [
    "### Summary from BART\n",
    "Encoder-Decoder architecture, trained to reconstruct corrupted inputs\n",
    "Exclusively fine-tuned on this task \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "977fd162-c5d7-4267-91d6-9fd099652d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc902d86ea24b34a8bad2922c5c1f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad4af8d9c3b4f01bac8f638d8cc3639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5c966b2f5e41f5839be30b6be433c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174d1944d6fe4cbe9aeb9f6fbc4784cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a8216539a5402abb02ae6e7aa10319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acbd0398e7a43528acfdf144546fa1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1434af-cdf5-4d63-996a-9e2c9e6d6a5a",
   "metadata": {},
   "source": [
    "### Summary from PEGASUS\n",
    "Predict masked sentences \n",
    "\n",
    "Exclusively fine-tuned on this task \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb453f0-1838-4b97-b88f-ef19d54049d8",
   "metadata": {},
   "source": [
    "Doesn't require sentence tokenizer as it already has a special token for newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e4318a4-1daf-4ccf-8190-548386f85833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20\n",
      "  Downloading protobuf-3.20.0.tar.gz (216 kB)\n",
      "     -------------------------------------- 216.5/216.5 KB 4.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Skipping wheel build for protobuf, due to binaries being disabled for it.\n",
      "Installing collected packages: protobuf\n",
      "  Running setup.py install for protobuf: started\n",
      "  Running setup.py install for protobuf: finished with status 'done'\n",
      "Successfully installed protobuf-3.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# !pip uninstall protobuf\n",
    "!pip install --no-binary protobuf protobuf==3.20\n",
    "# !pip install protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9361161d-0385-4b6e-94de-67da5c31d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9998a2c4-6c99-4bd0-b6d2-ca923d041eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mentally ill inmates in Miami are housed on the \"forgotten floor\"<n>The ninth floor is where they\\'re held until they\\'re ready to appear in court.\\nMost often, they face drug charges or charges of assaulting an officer.\\nThey end up on the ninth floor severely mentally disturbed .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[\"pegasus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa96e5-3aea-494d-920b-ddaf00508bbe",
   "metadata": {},
   "source": [
    "## Comparison of summaries generated from different models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeedea67-3750-4528-b70f-eba57dae86d4",
   "metadata": {},
   "source": [
    "1. GPT2 was used instead of GPT2-xl\n",
    "2. T5-small was used instead of T5-large\n",
    "3. BART-large-cnn was used as is\n",
    "4. PEGASUS-cnndailymail was used as is\n",
    "5. NOTE that\n",
    "    - gpt2 has never been trained on this dataset,\n",
    "    - T5 has been fine tuned on this task among other tasks like translation\n",
    "    - BART(combined pretraining scheme of BERT and GPT-2) and PEGASUS have been exclusively fine-tuned on this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70c778ea-0120-4627-960b-56324dfdfa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n",
      "\n",
      "BASELINE\n",
      "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events.\n",
      "Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial.\n",
      "MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\"\n",
      "\n",
      "GPT2\n",
      "\n",
      "A jail has been designated as \"forgotten.\"\n",
      "As we explained in our earlier post titled \"forgotten,\" inmates have to be removed from their cells at least three times a week if their condition deteriorates.\n",
      "But, what exactly qualifies as a \"forgotten floor\" means less than an hour to a day.\n",
      "Because of this, detainees tend to stay far more quiet and have\n",
      "\n",
      "T5\n",
      "inmates with most severe mental illnesses are incarcerated until they're ready to appear in court .\n",
      "most often, they face drug charges or charges of assaulting an officer .\n",
      "they end up on the ninth floor severely mentally disturbed, but not getting real help .\n",
      "\n",
      "BART\n",
      "Mentally ill inmates are housed on the \"forgotten floor\" of Miami-Dade jail.\n",
      "Most often, they face drug charges or charges of assaulting an officer.\n",
      "Judge Steven Leifman says the arrests often result from confrontations with police.\n",
      "He says about one-third of all people in the county jails are mentally ill.\n",
      "\n",
      "PEGASUS\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"<n>The ninth floor is where they're held until they're ready to appear in court.\n",
      "Most often, they face drug charges or charges of assaulting an officer.\n",
      "They end up on the ninth floor severely mentally disturbed .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "print(dataset[\"train\"][1][\"highlights\"])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39864e-7ff4-4e15-b48d-7932041c1b4f",
   "metadata": {},
   "source": [
    "##### REMARKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac06ea1-2399-4605-b2bd-627be8543dcd",
   "metadata": {},
   "source": [
    "1. Performance of all the models on the sample text\n",
    "    - PEGASUS model performs the best above, with BART and T5 doing better than GPT2\n",
    "3. How to choose the model for production setting\n",
    "    - All the models needs to be compared but more systematically, i.e. using some metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119cb37-0327-43f3-9d78-518ded2a4f3a",
   "metadata": {},
   "source": [
    "## Metrics to check the quality of the generated text\n",
    "\n",
    "- Exact match like classification or NER isn't feasible for text generation quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebee75-4964-49ac-9236-8ee9b4dbd066",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "- Look at n-grams instead of tokens\n",
    "- precision-based metric, counting correct occurrences as many times as it occurs in the reference text\n",
    "- Discourages recall influence to avoid all the words from all the references\n",
    "- Uses Brevity penalty to peanlise only smaller length text being generated\n",
    "- Mostly BLEU-4 score is used where upto 4-grams are considered in metric evaluation\n",
    "- Doesn't account for synonyms and mostly heuristics based, also it requires tokenisation to be done\n",
    "- Alternative called Sacrebleu has its own tokenisation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7291543a-af1d-4e0c-894d-d3178a87591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb678fe-abe3-45f7-85e1-bd1166cb90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_metrics, load_metric\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4b297d5-c5dd-449a-9e6d-352a46a3bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
      "Produces BLEU scores along with its sufficient statistics\n",
      "from a source against one or more references.\n",
      "\n",
      "Args:\n",
      "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
      "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
      "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
      "        - `'none'`: no smoothing\n",
      "        - `'floor'`: increment zero counts\n",
      "        - `'add-k'`: increment num/denom by k for n>1\n",
      "        - `'exp'`: exponential decay\n",
      "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
      "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
      "        - `'none'`: No tokenization.\n",
      "        - `'zh'`: Chinese tokenization.\n",
      "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
      "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
      "        - `'char'`: Language-agnostic character-level tokenization.\n",
      "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
      "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
      "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
      "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
      "\n",
      "Returns:\n",
      "    'score': BLEU score,\n",
      "    'counts': Counts,\n",
      "    'totals': Totals,\n",
      "    'precisions': Precisions,\n",
      "    'bp': Brevity penalty,\n",
      "    'sys_len': predictions length,\n",
      "    'ref_len': reference length,\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1:\n",
      "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
      "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
      "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
      "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
      "        >>> print(list(results.keys()))\n",
      "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "        >>> print(round(results[\"score\"], 1))\n",
      "        100.0\n",
      "\n",
      "    Example 2:\n",
      "        >>> predictions = [\"hello there general kenobi\",\n",
      "        ...                 \"on our way to ankh morpork\"]\n",
      "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
      "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
      "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
      "        >>> results = sacrebleu.compute(predictions=predictions,\n",
      "        ...                             references=references)\n",
      "        >>> print(list(results.keys()))\n",
      "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "        >>> print(round(results[\"score\"], 1))\n",
      "        39.8\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "print(bleu_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1176f047-8304-4e1d-9c22-e3b151b01d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>[4, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totals</th>\n",
       "      <td>[12, 10, 8, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisions</th>\n",
       "      <td>[33.333333333333336, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys_len</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref_len</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Value\n",
       "score                                       0.0\n",
       "counts                             [4, 0, 0, 0]\n",
       "totals                           [12, 10, 8, 6]\n",
       "precisions  [33.333333333333336, 0.0, 0.0, 0.0]\n",
       "bp                                          1.0\n",
       "sys_len                                      12\n",
       "ref_len                                      12"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_metric.add(prediction=\"the the the the the the\", \n",
    "                reference=[\"the cat is on the mat\"])\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    " bleu_metric.compute(smooth_method='floor', smooth_value=0),\n",
    " orient=\"index\", columns=[\"Value\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0306ba-65fa-4ea3-bba2-24ce0857f859",
   "metadata": {},
   "source": [
    "Some of the resultant attributes like precisions are list where each element corresponds to the correspondingn-gram\n",
    "\n",
    "For e.g., above, 33.33% is precision for 1-gram, and 0% for rest of the 2-grams, 3-gram and 4-grams\n",
    "\n",
    "As Bleu score by default is upto 4-grams only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd4c4b8b-2122-4663-8305-6501333281e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>57.893007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>[5, 3, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totals</th>\n",
       "      <td>[5, 4, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisions</th>\n",
       "      <td>[100.0, 75.0, 66.66666666666667, 50.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>0.818731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys_len</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Value\n",
       "score                                    57.893007\n",
       "counts                                [5, 3, 2, 1]\n",
       "totals                                [5, 4, 3, 2]\n",
       "precisions  [100.0, 75.0, 66.66666666666667, 50.0]\n",
       "bp                                        0.818731\n",
       "sys_len                                          5\n",
       "ref_len                                          6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_metric.add(prediction=\"the cat is on mat\", \n",
    "                reference=[\"the cat is on the mat\"])\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    " bleu_metric.compute(smooth_method='floor', smooth_value=0),\n",
    " orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9c70b80-4b52-4428-899c-433dc07c95fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\",\n",
       " 'Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he\\'s fighting for change .')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"highlights\"], dataset[\"train\"][1][\"highlights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a8e2d-e798-4284-8525-3e78710ee87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "for model_name in summaries:\n",
    " bleu_metric.add(prediction=summaries[model_name],reference=[reference])\n",
    " \n",
    " # NOTE that reference is passed as a list coz BLEU score can be computed \n",
    " # even if multiple references are available for a single translation\n",
    " \n",
    " score = bleu_metric.compute(smooth_method='floor', smooth_value=0)\n",
    " # print(model_name, score)   \n",
    " # bleu_dict = dict({model_name: score})\n",
    " records.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a51a84d-0f86-431e-bcb2-b992232f02ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>counts</th>\n",
       "      <th>totals</th>\n",
       "      <th>precisions</th>\n",
       "      <th>bp</th>\n",
       "      <th>sys_len</th>\n",
       "      <th>ref_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>8.379861</td>\n",
       "      <td>[28, 9, 4, 3]</td>\n",
       "      <td>[90, 89, 88, 87]</td>\n",
       "      <td>[31.11111111111111, 10.112359550561798, 4.5454...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>4.639980</td>\n",
       "      <td>[16, 4, 2, 1]</td>\n",
       "      <td>[74, 73, 72, 71]</td>\n",
       "      <td>[21.62162162162162, 5.47945205479452, 2.777777...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>74</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>[10, 1, 0, 0]</td>\n",
       "      <td>[47, 46, 45, 44]</td>\n",
       "      <td>[21.27659574468085, 2.1739130434782608, 0.0, 0.0]</td>\n",
       "      <td>0.808345</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>21.296284</td>\n",
       "      <td>[25, 13, 10, 6]</td>\n",
       "      <td>[57, 56, 55, 54]</td>\n",
       "      <td>[43.85964912280702, 23.214285714285715, 18.181...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>22.382122</td>\n",
       "      <td>[18, 12, 11, 10]</td>\n",
       "      <td>[56, 55, 54, 53]</td>\n",
       "      <td>[32.142857142857146, 21.818181818181817, 20.37...</td>\n",
       "      <td>0.982301</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score            counts            totals  \\\n",
       "baseline   8.379861     [28, 9, 4, 3]  [90, 89, 88, 87]   \n",
       "gpt2       4.639980     [16, 4, 2, 1]  [74, 73, 72, 71]   \n",
       "t5         0.000000     [10, 1, 0, 0]  [47, 46, 45, 44]   \n",
       "bart      21.296284   [25, 13, 10, 6]  [57, 56, 55, 54]   \n",
       "pegasus   22.382122  [18, 12, 11, 10]  [56, 55, 54, 53]   \n",
       "\n",
       "                                                 precisions        bp  \\\n",
       "baseline  [31.11111111111111, 10.112359550561798, 4.5454...  1.000000   \n",
       "gpt2      [21.62162162162162, 5.47945205479452, 2.777777...  1.000000   \n",
       "t5        [21.27659574468085, 2.1739130434782608, 0.0, 0.0]  0.808345   \n",
       "bart      [43.85964912280702, 23.214285714285715, 18.181...  1.000000   \n",
       "pegasus   [32.142857142857146, 21.818181818181817, 20.37...  0.982301   \n",
       "\n",
       "          sys_len  ref_len  \n",
       "baseline       90       57  \n",
       "gpt2           74       57  \n",
       "t5             47       57  \n",
       "bart           57       57  \n",
       "pegasus        56       57  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_df = pd.DataFrame.from_records(records, index=summaries.keys())\n",
    "bleu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfda9874-109f-4eb1-a740-57bac7b8955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def round_off(row):\n",
    "    new_list = []\n",
    "    for item in row[\"precisions\"]:\n",
    "        new_list.append(np.round(item, 2))\n",
    "    return new_list\n",
    "bleu_df[\"precisions_rounded_off\"]= bleu_df[[\"precisions\"]].apply(round_off, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "edc9b61a-2d83-4da4-83cb-47aa6f996e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precisions_rounded_off</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>[31.11, 10.11, 4.55, 3.45]</td>\n",
       "      <td>8.379861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>[21.62, 5.48, 2.78, 1.41]</td>\n",
       "      <td>4.639980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>[21.28, 2.17, 0.0, 0.0]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>[43.86, 23.21, 18.18, 11.11]</td>\n",
       "      <td>21.296284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>[32.14, 21.82, 20.37, 18.87]</td>\n",
       "      <td>22.382122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                precisions_rounded_off      score\n",
       "baseline    [31.11, 10.11, 4.55, 3.45]   8.379861\n",
       "gpt2         [21.62, 5.48, 2.78, 1.41]   4.639980\n",
       "t5             [21.28, 2.17, 0.0, 0.0]   0.000000\n",
       "bart      [43.86, 23.21, 18.18, 11.11]  21.296284\n",
       "pegasus   [32.14, 21.82, 20.37, 18.87]  22.382122"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_df[[\"precisions_rounded_off\", \"score\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39a0a3-7bcd-4f29-beee-20ef8a2f75a5",
   "metadata": {},
   "source": [
    "##### REMARK on the above precision scores\n",
    "for higher n-grams, pegaus is the clear winner\n",
    "\n",
    "For gpt2 and T5, we used their smaller versions \n",
    "\n",
    "Baseline score is not bad either!, considering the fact that it was just first 3 sentences of the original article\n",
    "\n",
    "Bart score is good only for smaller n-grams compared to pegasus score\n",
    "\n",
    "Pegasus has the highest BLEU SCore among all the LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c5d454-a3eb-46c0-be5c-df502d77c37d",
   "metadata": {},
   "source": [
    "##### REMARK on Bleu score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd558c7-7ce9-4911-9b76-5e458e988301",
   "metadata": {},
   "source": [
    "In general, precise translations are preferred over the translations that includes all possible words\n",
    "\n",
    "However for summarization, more than the words, important information should not be missed.\n",
    "\n",
    "hence higher recall for summarisation is good and a requirement => enters the ROUGE score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494e519-09aa-4c08-8243-f7c42330fc9c",
   "metadata": {},
   "source": [
    "### ROUGE score\n",
    "- Similar to BLEU score except the recall part\n",
    "- checks how many n-grams in the reference text also occurs in the generated text\n",
    "- This is in contrast to the BLEU where we looked the other way round\n",
    "- Developed specially for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c809b66c-f134-4443-a39a-7b838c36c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install absl\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ae333f-56b7-4349-a3ba-97c0f354fcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WMYFHCK\\AppData\\Local\\Temp\\ipykernel_7052\\3152983499.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric(\"rouge\", cache_dir=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_metric = load_metric(\"rouge\", cache_dir=None)\n",
    "rouge_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3fc1a8f1-b03a-4eb5-927a-2fd020e17171",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67da353d-e30e-4f97-ba6d-8f04ca156890",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "455e75df-24bc-4cec-b146-70991768e173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.144144</td>\n",
       "      <td>0.036697</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.126126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.415842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.206186</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.323232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.365079  0.145161  0.206349   0.285714\n",
       "gpt2      0.144144  0.036697  0.108108   0.126126\n",
       "t5        0.195652  0.022222  0.108696   0.173913\n",
       "bart      0.475248  0.222222  0.316832   0.415842\n",
       "pegasus   0.323232  0.206186  0.282828   0.323232"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for model_name in summaries:\n",
    " rouge_metric.add(prediction=summaries[model_name],reference=reference)\n",
    " score = rouge_metric.compute()\n",
    " rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    " records.append(rouge_dict)\n",
    "\n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82e065-40dd-42ef-8a14-4425c4a301ab",
   "metadata": {},
   "source": [
    "##### REMARK on pegasus scores from the above table\n",
    "\n",
    "For all the rouge scores, gp2-small's score are lowest (compared to T5-small,BART and Pegasus score)\n",
    "\n",
    "REMEMBER GPT2 was not explicitly trained to summarize\n",
    "\n",
    "Basleine score are close and comparable to large size LLMs (order of billion!) \n",
    "\n",
    "PEGASUS has only 500M parameters where as T5-large (not used here but referred in the textbook) has 11B parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb4083-ca2b-4d80-be40-b3d245f9bfa0",
   "metadata": {},
   "source": [
    "## Evaluating PEGASUS on entire CNN/DailyMail dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06716ce7-93ea-48e3-8a05-8dbc5ba47641",
   "metadata": {},
   "source": [
    "#### Baseline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dadf3410-e0e7-44b6-ae98-4ce06f402f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric,column_text=\"article\",column_summary=\"highlights\"):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
    "    metric.add_batch(predictions=summaries, references=dataset[column_summary])\n",
    "    score = metric.compute()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c76d5-7880-4088-ac1d-9d79449d1368",
   "metadata": {},
   "source": [
    "#### Sampling from Test Set\n",
    "\n",
    "Sample 1000 points from the testset instead of spending time on 10K entire set\n",
    "\n",
    "Even 1000 sample set will take time close to hour on a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9cec90bf-593b-4b35-a786-db40685bc6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at C:\\Users\\WMYFHCK\\.cache\\huggingface\\datasets\\cnn_dailymail\\default\\3.0.0\\1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\\cache-888acb9a2eb72e89.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.386229</td>\n",
       "      <td>0.16465</td>\n",
       "      <td>0.245916</td>\n",
       "      <td>0.352733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1   rouge2    rougeL  rougeLsum\n",
       "baseline  0.386229  0.16465  0.245916   0.352733"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns= [\"baseline\"]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fefbb674-dc48-4241-a14a-927fea7f80a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2097101d-3519-476c-9a39-39d3c5463731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "device = \"cuda\"#if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c55f9a12-a7f8-446d-a9d6-cbc41bacf9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size=16, device=device, \n",
    "                               column_text=\"article\", column_summary=\"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n",
    "        dct = tokenizer.batch_encode_plus(article_batch, max_length=1024, truncation=True, \n",
    "                                          padding=\"max_length\", return_tensors=\"pt\")\n",
    "        summaries = model.generate(input_ids=dct[\"input_ids\"].to(device), length_penalty=0.8,\n",
    "                                   attention_mask=dct[\"attention_mask\"].to(device), num_beams=8, max_length=128)\n",
    "        dec = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in summaries]\n",
    "        dec = [d.replace(\"<n>\", \" \") for d in dec]\n",
    "        metric.add_batch(predictions=dec, references=target_batch)\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69221084-f327-4cc9-9ac6-f77c834c809b",
   "metadata": {},
   "source": [
    "#### Explanation of the above fun\n",
    "\n",
    "1. Split the dataset(both predicted and reference text) into smaller batches using the function chunks\n",
    "2. For each batch, tokenize and use model's generate function to produce summaries using beam-search with 8 beams\n",
    "3. Replace special newline tokens\n",
    "4. Use pegasus metric to get the rouge score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b305e48b-9f77-4c52-8672-fc0cd43092b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = (PegasusForConditionalGeneration.from_pretrained(model_name).to(device))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1edf2036-9c2e-421b-b1ee-6f46bbf0e344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\AppData\\Local\\Temp\\ipykernel_44516\\4119065355.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\WMYFHCK\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_44516\\\\4119065355.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\modeling_utils.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1896</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">to</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1893 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" model has already been set to the correct devices and casted to the co</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1894 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1895 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1896 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().to(*args, **kwargs)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1897 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1898 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">half</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args):                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1899 │   │   # Checks if the model has been loaded in 8-bit</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">145</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">to</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1142 │   │   │   │   │   │   │   </span>non_blocking, memory_format=convert_to_format)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1143 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> t.to(device, dtype <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> t.is_floating_point() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> t.is_complex() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">No</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1144 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1145 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._apply(convert)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1146 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1147 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">register_full_backward_pre_hook</span>(                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1148 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">97</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_apply</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 794 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 795 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_apply</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, fn):                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 796 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> module <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.children():                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 797 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>module._apply(fn)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 798 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 799 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute_should_use_set_data</span>(tensor, tensor_applied):                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 800 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch._has_compatible_shallow_copy_type(tensor, tensor_applied):           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">97</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_apply</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 794 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 795 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_apply</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, fn):                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 796 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> module <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.children():                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 797 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>module._apply(fn)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 798 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 799 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute_should_use_set_data</span>(tensor, tensor_applied):                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 800 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch._has_compatible_shallow_copy_type(tensor, tensor_applied):           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_apply</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 817 │   │   │   # track autograd history of `param_applied`, so we have to use</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 818 │   │   │   # `with torch.no_grad():`</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 819 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 820 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>param_applied = fn(param)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 821 │   │   │   </span>should_use_set_data = compute_should_use_set_data(param, param_applied)       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 822 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> should_use_set_data:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 823 │   │   │   │   </span>param.data = param_applied                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">143</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">convert</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1140 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> convert_to_format <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> t.dim() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> (<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>):                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1141 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> t.to(device, dtype <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> t.is_floating_point() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> t.is_complex() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">els</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1142 │   │   │   │   │   │   │   </span>non_blocking, memory_format=convert_to_format)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1143 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> t.to(device, dtype <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> t.is_floating_point() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> t.is_complex() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">No</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1144 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1145 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._apply(convert)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1146 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">376.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.00</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.47</span> GiB \n",
       "already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> bytes free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.47</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory \n",
       "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\AppData\\Local\\Temp\\ipykernel_44516\\4119065355.py\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92m<module>\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\WMYFHCK\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_44516\\\\4119065355.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\modeling_utils.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mpy\u001b[0m:\u001b[94m1896\u001b[0m in \u001b[92mto\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1893 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m model has already been set to the correct devices and casted to the co\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1894 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1895 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1896 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m().to(*args, **kwargs)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1897 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1898 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mhalf\u001b[0m(\u001b[96mself\u001b[0m, *args):                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1899 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Checks if the model has been loaded in 8-bit\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m145\u001b[0m in \u001b[92mto\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1142 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mnon_blocking, memory_format=convert_to_format)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1143 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m t.to(device, dtype \u001b[94mif\u001b[0m t.is_floating_point() \u001b[95mor\u001b[0m t.is_complex() \u001b[94melse\u001b[0m \u001b[94mNo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1144 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1145 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._apply(convert)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1146 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1147 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_full_backward_pre_hook\u001b[0m(                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1148 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m7\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m97\u001b[0m in \u001b[92m_apply\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 794 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 795 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_apply\u001b[0m(\u001b[96mself\u001b[0m, fn):                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 796 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 797 \u001b[2m│   │   │   \u001b[0mmodule._apply(fn)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 798 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 799 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcompute_should_use_set_data\u001b[0m(tensor, tensor_applied):                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 800 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m7\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m97\u001b[0m in \u001b[92m_apply\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 794 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 795 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_apply\u001b[0m(\u001b[96mself\u001b[0m, fn):                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 796 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 797 \u001b[2m│   │   │   \u001b[0mmodule._apply(fn)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 798 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 799 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcompute_should_use_set_data\u001b[0m(tensor, tensor_applied):                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 800 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m8\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m20\u001b[0m in \u001b[92m_apply\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# track autograd history of `param_applied`, so we have to use\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 818 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# `with torch.no_grad():`\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 819 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 820 \u001b[2m│   │   │   │   \u001b[0mparam_applied = fn(param)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 821 \u001b[0m\u001b[2m│   │   │   \u001b[0mshould_use_set_data = compute_should_use_set_data(param, param_applied)       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 822 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m should_use_set_data:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 823 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mparam.data = param_applied                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m143\u001b[0m in \u001b[92mconvert\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1140 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m convert_to_format \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m t.dim() \u001b[95min\u001b[0m (\u001b[94m4\u001b[0m, \u001b[94m5\u001b[0m):                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1141 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m t.to(device, dtype \u001b[94mif\u001b[0m t.is_floating_point() \u001b[95mor\u001b[0m t.is_complex() \u001b[94mels\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1142 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mnon_blocking, memory_format=convert_to_format)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1143 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m t.to(device, dtype \u001b[94mif\u001b[0m t.is_floating_point() \u001b[95mor\u001b[0m t.is_complex() \u001b[94melse\u001b[0m \u001b[94mNo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1144 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1145 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._apply(convert)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1146 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m376.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m4.00\u001b[0m GiB total capacity; \u001b[1;36m3.47\u001b[0m GiB \n",
       "already allocated; \u001b[1;36m0\u001b[0m bytes free; \u001b[1;36m3.47\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory \n",
       "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=16, device=device)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32ac74-11bb-48a2-aaf6-81346950a4f0",
   "metadata": {},
   "source": [
    "Even 15GB GPU was not sufficient for the computation above for batch size of 16 and total sampes 100, \n",
    "\n",
    "for now we have left it as it is in the local system. \n",
    "\n",
    "Remember, Code is same, just need a hig end GPU or reduce the test-sample size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f2af8-c99a-4087-849e-95ff31d14f20",
   "metadata": {},
   "source": [
    "##### ReAttempted to compute rouge metric for pegasus on google colab with 15gb gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a9587-63fd-4ed2-bd12-aa6f69970d9a",
   "metadata": {},
   "source": [
    "I took the code relevant to PEGASUS, RougeMetric and Test Set sampling from the dataset\n",
    "\n",
    "and sampled only 100 test samples, gave batch size of 8 instead of 16, \n",
    "\n",
    "Refer to the notebook named TextSummarizationPegasus_googlecolab.ipynb\n",
    "\n",
    "We get the following results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f2970f4-20ce-4554-93a4-50f3ad7a6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_dict = {'rouge1': 0.42560994242964456,\n",
    " 'rouge2': 0.20219092151645182,\n",
    " 'rougeL': 0.2998548474626779,\n",
    " 'rougeLsum': 0.36100320427452914}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ba88606d-b477-4c63-b1e5-dede72fee1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.42561</td>\n",
       "      <td>0.202191</td>\n",
       "      <td>0.299855</td>\n",
       "      <td>0.361003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rouge1    rouge2    rougeL  rougeLsum\n",
       "pegasus  0.42561  0.202191  0.299855   0.361003"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns= [\"pegasus\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b18c9-c1bb-423f-ba50-7809586fae8c",
   "metadata": {},
   "source": [
    "##### REMARK\n",
    "These scores are very close to the scores for 1000 samples given in the textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8a795-451c-4864-ad04-9b3ace5cd606",
   "metadata": {},
   "source": [
    "# Train your custom summarization model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fde7e5-daed-4bd9-99c8-b639edd62b8b",
   "metadata": {},
   "source": [
    "#### Using SamSum dataset comprising of conversations and corresponding summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "682b3dd1-1550-40d0-98a8-0e3e456c7ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py7zr\n",
      "  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
      "     -------------------------------------- 66.4/66.4 KB 719.5 kB/s eta 0:00:00\n",
      "Collecting pycryptodomex>=3.6.6\n",
      "  Downloading pycryptodomex-3.18.0-cp35-abi3-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.6 MB/s eta 0:00:00\n",
      "Collecting pyzstd>=0.14.4\n",
      "  Downloading pyzstd-0.15.9-cp310-cp310-win_amd64.whl (245 kB)\n",
      "     ------------------------------------- 245.3/245.3 KB 15.7 MB/s eta 0:00:00\n",
      "Collecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting inflate64>=0.3.1\n",
      "  Downloading inflate64-0.3.1-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: texttable in c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages (from py7zr) (1.6.7)\n",
      "Requirement already satisfied: psutil in c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages (from py7zr) (5.9.5)\n",
      "Collecting pybcj>=0.6.0\n",
      "  Downloading pybcj-1.0.1-cp310-cp310-win_amd64.whl (24 kB)\n",
      "Collecting brotli>=1.0.9\n",
      "  Downloading Brotli-1.0.9-cp310-cp310-win_amd64.whl (383 kB)\n",
      "     ------------------------------------- 383.3/383.3 KB 23.3 MB/s eta 0:00:00\n",
      "Collecting pyppmd<1.1.0,>=0.18.1\n",
      "  Downloading pyppmd-1.0.0-cp310-cp310-win_amd64.whl (46 kB)\n",
      "     ---------------------------------------- 46.1/46.1 KB ? eta 0:00:00\n",
      "Installing collected packages: brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n",
      "Successfully installed brotli-1.0.9 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wmyfhck\\documents\\projects\\ml_py310_venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2970e43a-0436-41b6-864d-1bbddcd429f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (C:/Users/WMYFHCK/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a363d30f78d440590dffb034eb9e607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98f530-978d-49dc-a3c3-5fa700f1557b",
   "metadata": {},
   "source": [
    "### Evaluating PEGASUS(trained on CNN/ on SamSum dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4a69c8d2-2f91-4603-948c-6a7f1221e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "\n",
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3ea89713-4ad8-434d-9908-9c8afa50d944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Amanda: Ask Larry Amanda: He called her last time we were at the park together.\n",
      "Hannah: I'd rather you texted him.\n",
      "Amanda: Just text him .\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary:\")\n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e356ea-c23b-4fe9-a08c-4fdb91a35a04",
   "metadata": {},
   "source": [
    "##### PEGASUS trained on CNN/DailyMail gives is more like a extractive summary \n",
    "##### but we need abstractive summary for chat/text messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b943e2-5206-419d-8323-e6bf180a146e",
   "metadata": {},
   "source": [
    "#### Rouge Metric on PEGASUS summaries with Samsum summaries as the referenfces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09831aeb-1e57-4689-9307-c25a4e6f5230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                   | 0/103 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\AppData\\Local\\Temp\\ipykernel_7052\\1856661876.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\WMYFHCK\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_7052\\\\1856661876.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\AppData\\Local\\Temp\\ipykernel_7052\\1522300070.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluate_summaries_pegasus</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\WMYFHCK\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_7052\\\\1522300070.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\utils\\_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">15</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>115 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> decorate_context                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\generation\\util</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">s.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1524</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1521 │   │   │   │   </span>**model_kwargs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1522 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1523 │   │   │   # 13. run beam search</span>                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1524 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.beam_search(                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1525 │   │   │   │   </span>input_ids,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1526 │   │   │   │   </span>beam_scorer,                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1527 │   │   │   │   </span>logits_processor=logits_processor,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\generation\\util</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">s.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2810</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">beam_search</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2807 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2808 │   │   │   </span>model_inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prepare_inputs_for_generation(input_ids, **model_kwargs)  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2809 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2810 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2811 │   │   │   │   </span>**model_inputs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2812 │   │   │   │   </span>return_dict=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2813 │   │   │   │   </span>output_attentions=output_attentions,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">modeling_pegasus.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1416</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1413 │   │   │   │   │   </span>labels, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.pad_token_id, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.decoder_start_token_id  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1414 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1415 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1416 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1417 │   │   │   </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1418 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1419 │   │   │   </span>decoder_input_ids=decoder_input_ids,                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">modeling_pegasus.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1271</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1268 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1269 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1270 │   │   # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_att</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1271 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>decoder_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1272 │   │   │   </span>input_ids=decoder_input_ids,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1273 │   │   │   </span>attention_mask=decoder_attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1274 │   │   │   </span>encoder_hidden_states=encoder_outputs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>],                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">modeling_pegasus.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1103</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1100 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1101 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1102 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1103 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>layer_outputs = decoder_layer(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1104 │   │   │   │   │   </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1105 │   │   │   │   │   </span>attention_mask=attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1106 │   │   │   │   │   </span>encoder_hidden_states=encoder_hidden_states,                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">modeling_pegasus.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">452</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 449 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 450 │   │   │   # cross_attn cached key/values tuple is at positions 3,4 of present_key_valu</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 451 │   │   │   </span>cross_attn_past_key_value = past_key_value[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>:] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> past_key_value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">Non</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 452 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>hidden_states, cross_attn_weights, cross_attn_present_key_value = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encod  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 453 │   │   │   │   </span>hidden_states=hidden_states,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 454 │   │   │   │   </span>key_value_states=encoder_hidden_states,                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 455 │   │   │   │   </span>attention_mask=encoder_attention_mask,                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">modeling_pegasus.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">212</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 209 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> is_cross_attention:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 210 │   │   │   # cross_attentions</span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 211 │   │   │   </span>key_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._shape(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.k_proj(key_value_states), -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, bsz)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 212 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>value_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._shape(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.v_proj(key_value_states), -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, bsz)            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 213 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> past_key_value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 214 │   │   │   # reuse k, v, self_attention</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 215 │   │   │   </span>key_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._shape(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.k_proj(hidden_states), -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, bsz)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">modeling_pegasus.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">176</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_shape</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 173 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 174 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 175 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_shape</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, tensor: torch.Tensor, seq_len: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>, bsz: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>):                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 176 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> tensor.view(bsz, seq_len, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head_dim).transpose(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>).  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 177 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 178 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 179 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.00</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.39</span> GiB \n",
       "already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> bytes free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.41</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory \n",
       "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\AppData\\Local\\Temp\\ipykernel_7052\\1856661876.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\WMYFHCK\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_7052\\\\1856661876.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\AppData\\Local\\Temp\\ipykernel_7052\\1522300070.py\u001b[0m:\u001b[94m8\u001b[0m in \u001b[92mevaluate_summaries_pegasus\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\WMYFHCK\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_7052\\\\1522300070.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m15\u001b[0m in \u001b[92mdecorate_context\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\generation\\util\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33ms.py\u001b[0m:\u001b[94m1524\u001b[0m in \u001b[92mgenerate\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**model_kwargs,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1522 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# 13. run beam search\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1524 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.beam_search(                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1525 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1526 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbeam_scorer,                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1527 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits_processor=logits_processor,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\generation\\util\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33ms.py\u001b[0m:\u001b[94m2810\u001b[0m in \u001b[92mbeam_search\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2807 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2808 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_inputs = \u001b[96mself\u001b[0m.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2809 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2810 \u001b[2m│   │   │   \u001b[0moutputs = \u001b[96mself\u001b[0m(                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2811 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**model_inputs,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2812 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mreturn_dict=\u001b[94mTrue\u001b[0m,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2813 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput_attentions=output_attentions,                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m501\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mmodeling_pegasus.py\u001b[0m:\u001b[94m1416\u001b[0m in \u001b[92mforward\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1413 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlabels, \u001b[96mself\u001b[0m.config.pad_token_id, \u001b[96mself\u001b[0m.config.decoder_start_token_id  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1414 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1415 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1416 \u001b[2m│   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.model(                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1417 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1418 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1419 \u001b[0m\u001b[2m│   │   │   \u001b[0mdecoder_input_ids=decoder_input_ids,                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m501\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mmodeling_pegasus.py\u001b[0m:\u001b[94m1271\u001b[0m in \u001b[92mforward\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1268 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1269 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1270 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_att\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1271 \u001b[2m│   │   \u001b[0mdecoder_outputs = \u001b[96mself\u001b[0m.decoder(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1272 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids=decoder_input_ids,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1273 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=decoder_attention_mask,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1274 \u001b[0m\u001b[2m│   │   │   \u001b[0mencoder_hidden_states=encoder_outputs[\u001b[94m0\u001b[0m],                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m501\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mmodeling_pegasus.py\u001b[0m:\u001b[94m1103\u001b[0m in \u001b[92mforward\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1100 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mNone\u001b[0m,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1101 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1102 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1103 \u001b[2m│   │   │   │   \u001b[0mlayer_outputs = decoder_layer(                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1104 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states,                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1105 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask=attention_mask,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1106 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mencoder_hidden_states=encoder_hidden_states,                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m501\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mmodeling_pegasus.py\u001b[0m:\u001b[94m452\u001b[0m in \u001b[92mforward\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 449 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 450 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# cross_attn cached key/values tuple is at positions 3,4 of present_key_valu\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 451 \u001b[0m\u001b[2m│   │   │   \u001b[0mcross_attn_past_key_value = past_key_value[-\u001b[94m2\u001b[0m:] \u001b[94mif\u001b[0m past_key_value \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNon\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 452 \u001b[2m│   │   │   \u001b[0mhidden_states, cross_attn_weights, cross_attn_present_key_value = \u001b[96mself\u001b[0m.encod  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 453 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhidden_states=hidden_states,                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 454 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mkey_value_states=encoder_hidden_states,                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 455 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mattention_mask=encoder_attention_mask,                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m501\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mmodeling_pegasus.py\u001b[0m:\u001b[94m212\u001b[0m in \u001b[92mforward\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 209 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m is_cross_attention:                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 210 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# cross_attentions\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 211 \u001b[0m\u001b[2m│   │   │   \u001b[0mkey_states = \u001b[96mself\u001b[0m._shape(\u001b[96mself\u001b[0m.k_proj(key_value_states), -\u001b[94m1\u001b[0m, bsz)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 212 \u001b[2m│   │   │   \u001b[0mvalue_states = \u001b[96mself\u001b[0m._shape(\u001b[96mself\u001b[0m.v_proj(key_value_states), -\u001b[94m1\u001b[0m, bsz)            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 213 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m past_key_value \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 214 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# reuse k, v, self_attention\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 215 \u001b[0m\u001b[2m│   │   │   \u001b[0mkey_states = \u001b[96mself\u001b[0m._shape(\u001b[96mself\u001b[0m.k_proj(hidden_states), -\u001b[94m1\u001b[0m, bsz)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\models\\pegasus\\\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mmodeling_pegasus.py\u001b[0m:\u001b[94m176\u001b[0m in \u001b[92m_shape\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 173 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 174 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 175 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_shape\u001b[0m(\u001b[96mself\u001b[0m, tensor: torch.Tensor, seq_len: \u001b[96mint\u001b[0m, bsz: \u001b[96mint\u001b[0m):                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 176 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m tensor.view(bsz, seq_len, \u001b[96mself\u001b[0m.num_heads, \u001b[96mself\u001b[0m.head_dim).transpose(\u001b[94m1\u001b[0m, \u001b[94m2\u001b[0m).  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 177 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 178 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 179 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m256.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m4.00\u001b[0m GiB total capacity; \u001b[1;36m3.39\u001b[0m GiB \n",
       "already allocated; \u001b[1;36m0\u001b[0m bytes free; \u001b[1;36m3.41\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory \n",
       "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model,\n",
    "                                   tokenizer, column_text=\"dialogue\",\n",
    "                                   column_summary=\"summary\", batch_size=8)\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9d2e0-3989-4aa3-9f46-feade9f9fcc9",
   "metadata": {},
   "source": [
    "4GB GPU is not sufficient for the above computation, I took this to google colab's 15GB GPU to get the following results in about 20+ minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf4151-ba39-4097-8c4d-f0365c50cf4b",
   "metadata": {},
   "source": [
    "Refer to the notebook named TextSummarizationPegasus_googlecolab.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "056d844a-4772-46b9-a9d0-353077ff6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_dict = {'rouge1': 0.2959613863327386,\n",
    " 'rouge2': 0.08774986612703142,\n",
    " 'rougeL': 0.22921461510956825,\n",
    " 'rougeLsum': 0.2293541143118824}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bca77247-20a8-4ae5-a432-0ebf41ed4238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.295961</td>\n",
       "      <td>0.08775</td>\n",
       "      <td>0.229215</td>\n",
       "      <td>0.229354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rouge1   rouge2    rougeL  rougeLsum\n",
       "pegasus  0.295961  0.08775  0.229215   0.229354"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7668546-47af-480c-bc70-4dc1458f9212",
   "metadata": {},
   "source": [
    "Data distribution of CNN/DailyMail is different from these chat like conversations summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092ef39-57e5-4c38-846b-ede988e9683a",
   "metadata": {},
   "source": [
    "##### EValuating in the beginning with some pretrained model like PEGASUS has advantages\n",
    "1. Good baseline\n",
    "2. Fine tuning on the contextual dataset will result in immediate imrovement\n",
    "3. If improvement not happening, something must be wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c2871-4501-44eb-849a-31d97957a878",
   "metadata": {},
   "source": [
    "### Fine tuning PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a14656-8624-4954-a9d5-c9a38d64c015",
   "metadata": {},
   "source": [
    "#### Length of the input and output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d79419b8-46c7-44fe-8579-6fa8e3a89e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c3e0a00-cea0-4997-9b1d-7c75b7a3475a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x350 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\n",
    "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "axes[0].set_title(\"Dialogue Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "axes[1].set_title(\"Summary Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1964989-0b92-4471-8b8d-6ed6ac564813",
   "metadata": {},
   "source": [
    "Most of the dialogues are in the range of 100s while summary is in the range of 20 to 30 tokens\n",
    "\n",
    "Above analysis is useful for data collator to pad/truncate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5326ce4-5461-4113-80df-7e93645d7fdd",
   "metadata": {},
   "source": [
    "1. Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98bca83a-e742-4983-b812-c900080a4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\r\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024,\r\n",
    "                                truncation=True)\r\n",
    "\r\n",
    "    with tokenizer.as_target_tokenizer():\r\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,\r\n",
    "                                     truncation=True)\r\n",
    "\r\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\r\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\r\n",
    "            \"labels\": target_encodings[\"input_ids\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9246a70-37d0-49e1-abb8-e2ca285a8e7e",
   "metadata": {},
   "source": [
    "##### tokenizer.as_target_tokenizer()\n",
    "Some models require special tokens in the decoder inputs, so it’s important to differentiate between the tokenization of encoder and decoder inputs. In the with statement (called a context manager), the tokenizer knows that it is tokenizing for the decoder and can process sequences accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd5c6d05-797e-43d2-aa2a-9ece96c3aa70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features,\n",
    "                                       batched=True)\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b792c-6a70-426b-b6a5-be25e1a2326e",
   "metadata": {},
   "source": [
    "2. Data collator\n",
    "\n",
    "Padding, shifting labels by one position to not see current or future tokens etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93f5f5c3-532a-4555-851b-e39e833e43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf594f9-9910-484f-a1ba-3d2c8974fa34",
   "metadata": {},
   "source": [
    "3. Trainer Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "399b1038-f7a2-40ea-b304-6b09ba0af801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10, push_to_hub=True,\n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dbebbd-2809-411e-8ad1-7314dc0379be",
   "metadata": {},
   "source": [
    "Model is quite big and hence we set batch size as 1 hurting the convergence. To handle this slow convergence, \n",
    "\n",
    "we set gradient_accumulation_steps argument\n",
    "\n",
    "In gradient accumulation gradient updates, instead of calculating the gradients of the full batch all at once, we make smaller batches and aggregate the gradients. When we have aggregated enough gradients, we run the optimization step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1f30f-425e-410e-b893-d15e57c3a8af",
   "metadata": {},
   "source": [
    "4. Hugging Face login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd39ea4f-347b-4893-9656-81c68cb3737c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ffc2b03f974063a01b2d827225dabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebf9f8-6d7d-44fe-b008-74d39d6a9134",
   "metadata": {},
   "source": [
    "5. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1823482-5c23-4ba0-949e-fd93288ce693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/pradeepiisc/pegasus-samsum into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "730a8bfd-55d1-4717-a52a-7f797edf7020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\AppData\\Local\\Temp\\ipykernel_7052\\49973641.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\WMYFHCK\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_7052\\\\49973641.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1662</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1659 │   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1660 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1662 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 │   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1664 │   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 │   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1929</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1926 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1927 │   │   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1928 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1929 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1930 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1931 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1932 │   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2717</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2714 │   │   │   # loss gets scaled under gradient_accumulation_steps in deepspeed</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2715 │   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed.backward(loss)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2716 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2717 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2718 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2719 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss.detach()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2720 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\autograd\\__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">00</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.00</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.40</span> GiB already\n",
       "allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> bytes free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.44</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\AppData\\Local\\Temp\\ipykernel_7052\\49973641.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\WMYFHCK\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_7052\\\\49973641.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\trainer.py\u001b[0m:\u001b[94m1662\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mtrain\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1662 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\trainer.py\u001b[0m:\u001b[94m1929\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1926 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1927 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1928 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1929 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1930 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1931 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1932 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\transformers\\trainer.py\u001b[0m:\u001b[94m2717\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mtraining_step\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2714 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# loss gets scaled under gradient_accumulation_steps in deepspeed\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2715 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.deepspeed.backward(loss)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2716 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2717 \u001b[2m│   │   │   \u001b[0mloss.backward()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2718 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2719 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m loss.detach()                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2720 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mbackward\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\WMYFHCK\\Documents\\Projects\\ml_py310_venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m:\u001b[94m2\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m00\u001b[0m in \u001b[92mbackward\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m20.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m4.00\u001b[0m GiB total capacity; \u001b[1;36m3.40\u001b[0m GiB already\n",
       "allocated; \u001b[1;36m0\u001b[0m bytes free; \u001b[1;36m3.44\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad4146-71b3-4928-b75c-115a4ea84cbd",
   "metadata": {},
   "source": [
    "##### Anyway simple inference was not happening on 4gb gpu, training is far away thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e2dc5-5fc5-408f-8a35-dae4fb44b019",
   "metadata": {},
   "source": [
    "##### WE will try to train the same on google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b51aa7-5093-4071-bd19-71045eb32738",
   "metadata": {},
   "source": [
    "##### It took more than 31minutes on google colab to train and more than 12 minutes to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5697704-8507-4077-9ba5-47ac75d9d6fd",
   "metadata": {},
   "source": [
    "##### Refer to the notebook named TextSummarizationPegasus_googlecolab.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe29922-3473-45e3-b632-dad44eab0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_summaries_pegasus(\n",
    "    dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer,\n",
    "    batch_size=2, column_text=\"dialogue\", column_summary=\"summary\")\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[f\"pegasus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe18e1-14b5-4247-9c12-c0e5870af859",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa43a37-0902-48dd-ae5e-745757a167a1",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
